{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Install & Import necessary libraries**"
      ],
      "metadata": {
        "id": "-lKuJglfdMcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "!pip install torch torchvision tqdm imageio\n",
        "!pip install torchinfo\n"
      ],
      "metadata": {
        "id": "zJFp8tNddVJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import imageio\n",
        "import itertools\n",
        "from google.colab import files\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchinfo import summary\n",
        "import timm\n"
      ],
      "metadata": {
        "id": "OluFHW22dXP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed()\n"
      ],
      "metadata": {
        "id": "Mva9vayjdaDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "6LoSAXmJdaF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Kaggle API Configuration**"
      ],
      "metadata": {
        "id": "glZzzOxZdhqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt the user to upload the kaggle.json file\n",
        "print(\"Please upload your kaggle.json file.\")\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "HZSBbvWKdaIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the .kaggle directory and move the kaggle.json file\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "qDor5xjGdkPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Download and Extract the Dataset**"
      ],
      "metadata": {
        "id": "SIUH_1hRdp8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dataset URL\n",
        "dataset_url = \"leslietiong/cmfpdb\"\n"
      ],
      "metadata": {
        "id": "PEGcgP4kdkSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!kaggle datasets download -d {dataset_url}\n",
        "\n",
        "# Unzip the dataset\n",
        "!unzip -q cmfpdb.zip -d ./cmfpdb\n"
      ],
      "metadata": {
        "id": "xwLsdJDtds7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Dataset Structure:**\n",
        "```\n",
        "Cross-modal Face-Periocular Dataset/\n",
        "|---- Hispanic/\n",
        "|---- |---- Alice\n",
        "|---- |---- |---- face\n",
        "|---- |---- |---- |---- 10.jpg\n",
        "|---- |---- |---- |---- 105.jpg\n",
        "|---- |---- |---- |---- ......\n",
        "|---- |---- |---- Ocular_left\n",
        "|---- |---- |---- |---- 10.jpg\n",
        "|---- |---- |---- |---- 105.jpg\n",
        "|---- |---- |---- |---- ......\n",
        "|---- |---- |---- Ocular_right\n",
        "|---- |---- |---- |---- 10.jpg\n",
        "|---- |---- |---- |---- 105.jpg\n",
        "|---- |---- |---- |---- ......\n",
        "|---- |---- ....\n",
        "|---- East Asian/\n",
        "|---- South Asian/\n",
        "|---- Caucasian/\n",
        "|---- Middle Eastern/\n",
        "|---- Melanesian/\n",
        "|---- African/\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "rg12GVgxduip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preprocess the Data**\n",
        "\n",
        "\n",
        "*   Label-1: same person, same region\n",
        "*   Label-0: different person, same region"
      ],
      "metadata": {
        "id": "1TbBzpIHdzA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = '/content/cmfpdb/Cross-modal Face-Periocular Dataset'\n",
        "\n",
        "person_images = {}  # {person_id: [image_paths]}\n",
        "region_persons = {}  # {region: [person_ids]}\n",
        "\n",
        "person_id_counter = 0\n",
        "subfolder = 'face'\n",
        "\n",
        "# Traverse the dataset directory\n",
        "print(\"Collecting image paths...\")\n",
        "for region in os.listdir(dataset_dir):\n",
        "    region_path = os.path.join(dataset_dir, region)\n",
        "    if os.path.isdir(region_path):\n",
        "        region_persons[region] = []\n",
        "        for person in os.listdir(region_path):\n",
        "            person_path = os.path.join(region_path, person)\n",
        "            if os.path.isdir(person_path):\n",
        "                person_id = f\"{region}_{person_id_counter}\"\n",
        "                person_id_counter += 1\n",
        "                images = []\n",
        "                subfolder_path = os.path.join(person_path, subfolder)\n",
        "                if os.path.exists(subfolder_path):\n",
        "                    for img_file in os.listdir(subfolder_path):\n",
        "                        if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                            img_path = os.path.join(subfolder_path, img_file)\n",
        "                            images.append(img_path)\n",
        "                if len(images) >= 2:\n",
        "                    person_images[person_id] = images\n",
        "                    region_persons[region].append(person_id)\n",
        "print(\"Finished collecting image paths.\")\n",
        "\n",
        "\n",
        "print(\"Generating Pairs...\")\n",
        "all_pairs = []\n",
        "\n",
        "# Create same-person pairs (100 per person)\n",
        "for person_id, images in tqdm(person_images.items(), desc=\"Generating Same-Person Pairs\"):\n",
        "    possible_pairs = list(itertools.combinations(images, 2))\n",
        "    if len(possible_pairs) > 100:\n",
        "        selected_pairs = random.sample(possible_pairs, 100)\n",
        "    else:\n",
        "        selected_pairs = possible_pairs\n",
        "\n",
        "    for img1, img2 in selected_pairs:\n",
        "        all_pairs.append([img1, img2, '1'])\n",
        "\n",
        "# Create different-person pairs (100 per person)\n",
        "for person_id, images in tqdm(person_images.items(), desc=\"Generating Different-Person Pairs\"):\n",
        "    other_person_ids = list(person_images.keys())\n",
        "    other_person_ids.remove(person_id)\n",
        "\n",
        "    selected_diff_pairs = []\n",
        "    while len(selected_diff_pairs) < 100:\n",
        "        other_person_id = random.choice(other_person_ids)\n",
        "        other_person_images = person_images[other_person_id]\n",
        "        other_img = random.choice(other_person_images)\n",
        "\n",
        "        img1 = random.choice(images)\n",
        "        selected_diff_pairs.append([img1, other_img, '0'])\n",
        "\n",
        "    all_pairs.extend(selected_diff_pairs)\n",
        "\n",
        "print(f\"Total pairs generated: {len(all_pairs)}\")\n",
        "\n",
        "output_csv = 'image_pairs_face.csv'\n",
        "print(f\"Writing pairs to {output_csv}...\")\n",
        "with open(output_csv, 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(['Image1', 'Image2', 'Label'])\n",
        "    csvwriter.writerows(all_pairs)\n",
        "\n",
        "print(\"Done!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "DLm_y-b7ds-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the existing CSV file\n",
        "face_csv = '/content/image_pairs_face.csv'\n",
        "df_face = pd.read_csv(face_csv)\n",
        "\n",
        "# Function to generate new CSV by replacing 'face' with the specified subfolder\n",
        "def generate_ocular_csv(subfolder_name):\n",
        "    df_ocular = df_face.copy()\n",
        "    df_ocular['Image1'] = df_ocular['Image1'].str.replace('/face/', f'/{subfolder_name}/')\n",
        "    df_ocular['Image2'] = df_ocular['Image2'].str.replace('/face/', f'/{subfolder_name}/')\n",
        "\n",
        "    existing_rows = []\n",
        "    for index, row in df_ocular.iterrows():\n",
        "        img1_path = row['Image1']\n",
        "        img2_path = row['Image2']\n",
        "        if os.path.exists(img1_path) and os.path.exists(img2_path):\n",
        "            existing_rows.append(row)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    df_existing = pd.DataFrame(existing_rows)\n",
        "    output_csv = f'image_pairs_{subfolder_name}.csv'\n",
        "    df_existing.to_csv(output_csv, index=False)\n",
        "    print(f\"Generated {output_csv} with {len(df_existing)} valid pairs.\")\n",
        "\n",
        "# Generate 'image_pairs_ocular_left.csv' and 'image_pairs_ocular_right.csv'\n",
        "generate_ocular_csv('ocular_left')\n",
        "generate_ocular_csv('ocular_right')\n"
      ],
      "metadata": {
        "id": "TeEXhXf8dtA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Define Dataset and DataLoader**"
      ],
      "metadata": {
        "id": "xQxmldoCd6tB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transformation for face images\n",
        "transform_face = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Define the transformation for ocular images\n",
        "transform_eyes = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n"
      ],
      "metadata": {
        "id": "LG9x2LSodtDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SiameseDataset(Dataset):\n",
        "    def __init__(self, face_df, ocular_left_df, ocular_right_df, transform_face=None, transform_eyes=None):\n",
        "        self.face_df = face_df\n",
        "        self.ocular_left_df = ocular_left_df\n",
        "        self.ocular_right_df = ocular_right_df\n",
        "        self.transform_face = transform_face\n",
        "        self.transform_eyes = transform_eyes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.face_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        person1_face = Image.open(self.face_df['Image1'][idx]).convert('RGB')\n",
        "        person1_ocular_left = Image.open(self.ocular_left_df['Image1'][idx]).convert('RGB')\n",
        "        person1_ocular_right = Image.open(self.ocular_right_df['Image1'][idx]).convert('RGB')\n",
        "        person2_face = Image.open(self.face_df['Image2'][idx]).convert('RGB')\n",
        "        person2_ocular_left = Image.open(self.ocular_left_df['Image2'][idx]).convert('RGB')\n",
        "        person2_ocular_right = Image.open(self.ocular_right_df['Image2'][idx]).convert('RGB')\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform_face:\n",
        "            person1_face = self.transform_face(person1_face)\n",
        "            person2_face = self.transform_face(person2_face)\n",
        "        if self.transform_eyes:\n",
        "            person1_ocular_left = self.transform_eyes(person1_ocular_left)\n",
        "            person1_ocular_right = self.transform_eyes(person1_ocular_right)\n",
        "            person2_ocular_left = self.transform_eyes(person2_ocular_left)\n",
        "            person2_ocular_right = self.transform_eyes(person2_ocular_right)\n",
        "\n",
        "        # Create label (1 for same person, 0 for different person)\n",
        "        label = torch.tensor(int(self.face_df['Label'][idx]), dtype=torch.float32)\n",
        "\n",
        "        return (person1_face, person1_ocular_left, person1_ocular_right,\n",
        "                person2_face, person2_ocular_left, person2_ocular_right, label)\n"
      ],
      "metadata": {
        "id": "HLYAa2RFdtGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV Files\n",
        "face_df = pd.read_csv('/content/image_pairs_face.csv')\n",
        "ocular_left_df = pd.read_csv('/content/image_pairs_ocular_left.csv')\n",
        "ocular_right_df = pd.read_csv('/content/image_pairs_ocular_right.csv')\n",
        "\n",
        "# Create the dataset/dataloader\n",
        "siamese_dataset = SiameseDataset(face_df, ocular_left_df, ocular_right_df,\n",
        "                                 transform_face=transform_face,\n",
        "                                 transform_eyes=transform_eyes)\n"
      ],
      "metadata": {
        "id": "h8O_1YKIdtJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset\n",
        "train_size = int(0.7 * len(siamese_dataset))\n",
        "val_size = int(0.15 * len(siamese_dataset))\n",
        "test_size = len(siamese_dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    siamese_dataset, [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42)  # Ensure reproducibility\n",
        ")\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "num_workers = 0\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n"
      ],
      "metadata": {
        "id": "3SS7Tt3YeE81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Define the Model (Siamese mViT)**"
      ],
      "metadata": {
        "id": "HAkU1GNmePJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MobileViTEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim=256):\n",
        "        super(MobileViTEncoder, self).__init__()\n",
        "        # Initialize MobileViT model from timm\n",
        "        self.mobilevit = timm.create_model('mobilevit_xxs', pretrained=True, num_classes=0)\n",
        "        self.feature_dim = self.mobilevit.num_features\n",
        "        self.fc = nn.Linear(self.feature_dim * 3, embedding_dim)\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "    def forward(self, face, ocular_left, ocular_right):\n",
        "        face_feat = self.mobilevit(face)\n",
        "        ocular_left_feat = self.mobilevit(ocular_left)\n",
        "        ocular_right_feat = self.mobilevit(ocular_right)\n",
        "\n",
        "        # Concatenate features from all three regions\n",
        "        combined = torch.cat([face_feat, ocular_left_feat, ocular_right_feat], dim=1)  # Shape: (batch_size, feature_dim*3)\n",
        "\n",
        "        embedding = F.relu(self.fc(combined))\n",
        "\n",
        "        return embedding\n",
        "\n",
        "\n",
        "\n",
        "class SiameseMobileViTFace(nn.Module):\n",
        "    def __init__(self, encoder, hidden_dim=512):\n",
        "        super(SiameseMobileViTFace, self).__init__()\n",
        "        self.encoder = encoder  # Shared encoder\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.fc1 = nn.Linear(encoder.embedding_dim, self.hidden_dim)\n",
        "        self.fc2 = nn.Linear(self.hidden_dim, 1)\n",
        "\n",
        "    def forward(self, face1, ocular_left1, ocular_right1, face2, ocular_left2, ocular_right2):\n",
        "        emb1 = self.encoder(face1, ocular_left1, ocular_right1)\n",
        "        emb2 = self.encoder(face2, ocular_left2, ocular_right2)\n",
        "\n",
        "        diff = torch.abs(emb1 - emb2)\n",
        "\n",
        "        x = F.relu(self.fc1(diff))\n",
        "        logits = self.fc2(x)\n",
        "\n",
        "        return logits.squeeze(1)\n",
        "\n"
      ],
      "metadata": {
        "id": "gES-VtgeeK4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training the Model**"
      ],
      "metadata": {
        "id": "Wnsl6LQpeToO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "zRBBhRnCeK8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n"
      ],
      "metadata": {
        "id": "JrAH-cf6h0ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs=20, lr=1e-4):\n",
        "    model.to(device)\n",
        "\n",
        "    labels = train_loader.dataset.dataset.face_df['Label']\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
        "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "    pos_weight = class_weights[1] / class_weights[0]\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight).to(device))\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "    best_val_acc = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "            face1, ocular_left1, ocular_right1, face2, ocular_left2, ocular_right2, labels = batch\n",
        "            face1, ocular_left1, ocular_right1 = face1.to(device), ocular_left1.to(device), ocular_right1.to(device)\n",
        "            face2, ocular_left2, ocular_right2 = face2.to(device), ocular_left2.to(device), ocular_right2.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(face1, ocular_left1, ocular_right1, face2, ocular_left2, ocular_right2)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        print(f\"Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Validation step\n",
        "        val_acc, val_loss = evaluate_model(model, val_loader, device, criterion)\n",
        "        print(f\"Validation Accuracy: {val_acc:.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save the best model based on validation accuracy\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), \"best_siamese_mobilevitface_model.pth\")\n",
        "            model_path_drive = '/content/drive/My Drive/Research/FacialRecognition/best_siamese_mobilevitface_model.pth'\n",
        "            torch.save(model.state_dict(), model_path_drive)\n",
        "            print(f\"Best model saved to Google Drive at: {model_path_drive}\")\n",
        "\n",
        "    print(\"Training Complete!\")\n",
        "\n",
        "def evaluate_model(model, data_loader, device, criterion):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    running_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            face1, ocular_left1, ocular_right1, face2, ocular_left2, ocular_right2, labels = batch\n",
        "            face1, ocular_left1, ocular_right1 = face1.to(device), ocular_left1.to(device), ocular_right1.to(device)\n",
        "            face2, ocular_left2, ocular_right2 = face2.to(device), ocular_left2.to(device), ocular_right2.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(face1, ocular_left1, ocular_right1, face2, ocular_left2, ocular_right2)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            preds = torch.sigmoid(outputs) >= 0.5  # Threshold at 0.5\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    avg_loss = running_loss / len(data_loader)\n",
        "    return accuracy, avg_loss\n",
        "\n",
        "def evaluate_on_test_set(model, test_loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "            face1, ocular_left1, ocular_right1, face2, ocular_left2, ocular_right2, labels = batch\n",
        "            face1, ocular_left1, ocular_right1 = face1.to(device), ocular_left1.to(device), ocular_right1.to(device)\n",
        "            face2, ocular_left2, ocular_right2 = face2.to(device), ocular_left2.to(device), ocular_right2.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(face1, ocular_left1, ocular_right1, face2, ocular_left2, ocular_right2)\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            preds = (probs >= 0.5).int()\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Test Precision: {precision:.4f}\")\n",
        "    print(f\"Test Recall: {recall:.4f}\")\n",
        "    print(f\"Test F1-Score: {f1:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Different', 'Same'],\n",
        "                yticklabels=['Different', 'Same'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot ROC Curve\n",
        "    plot_roc_curve(all_labels, all_probs)\n",
        "\n",
        "def plot_roc_curve(labels, probs):\n",
        "    fpr, tpr, thresholds = roc_curve(labels, probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.plot(fpr, tpr, color='darkorange',\n",
        "             lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "J6KXzVfXeXM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "# del variable_name\n"
      ],
      "metadata": {
        "id": "Ib_qOPyVea4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize MobileViT Encoder\n",
        "encoder = MobileViTEncoder(embedding_dim=256)\n",
        "\n",
        "# Initialize Siamese MobileViTFace Model\n",
        "model = SiameseMobileViTFace(encoder=encoder, hidden_dim=512)\n",
        "\n",
        "summary(model, input_size=[\n",
        "    (1, 3, 128, 128),  # face1\n",
        "    (1, 3, 128, 128),  # ocular_left1\n",
        "    (1, 3, 128, 128),  # ocular_right1\n",
        "    (1, 3, 128, 128),  # face2\n",
        "    (1, 3, 128, 128),  # ocular_left2\n",
        "    (1, 3, 128, 128),  # ocular_right2\n",
        "])\n"
      ],
      "metadata": {
        "id": "1w83blv7ea7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "daozy96R_XmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Start training**"
      ],
      "metadata": {
        "id": "YjCDC5u6eiKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "train_model(model, train_loader, val_loader, num_epochs=20, lr=1e-4)\n"
      ],
      "metadata": {
        "id": "espXAI7FekbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F-yF4PWzQd3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load trained Model**"
      ],
      "metadata": {
        "id": "K9pNbidSenji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_siamese_model(model_path, device):\n",
        "    encoder = MobileViTEncoder(embedding_dim=256)\n",
        "    model = SiameseMobileViTFace(encoder=encoder, hidden_dim=512)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Specify the path to the best model saved on Google Drive\n",
        "model_path = '/content/drive/My Drive/Research/FacialRecognition/best_siamese_mobilevitface_model.pth'\n",
        "\n",
        "# Load the model\n",
        "model = load_siamese_model(model_path, device)\n",
        "print(\"Model loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "cyr5jL5LeowQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L6eMUHwCeozC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation**"
      ],
      "metadata": {
        "id": "yUeNMcA3eudt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_on_test_set(model, test_loader, device)\n"
      ],
      "metadata": {
        "id": "H41MrvK8eu_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JvJN93_Z_GGJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}